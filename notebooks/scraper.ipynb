{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T06:48:14.571584Z",
     "start_time": "2024-04-04T06:48:14.568350Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:48:14.578986Z",
     "start_time": "2024-04-04T06:48:14.576142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_page = 1\n",
    "end_page = 3131 \n",
    "base_url = 'https://poradna.finance.cz'\n",
    "csv_filename = '../data/questions_and_answers.csv'\n",
    "csv_headers = ['Section', 'Theme', 'Question Date', 'Question Title', 'Question Text', 'QA Link', 'Responder Name', 'Answer Date', 'Answer Text']"
   ],
   "id": "b0391e65299c225a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:52:50.430995Z",
     "start_time": "2024-04-04T06:48:14.580497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_exists = os.path.isfile(csv_filename)\n",
    "mode = 'a' if file_exists else 'w'\n",
    "with open(csv_filename, mode, newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(csv_headers)\n",
    "    writer.writerow(csv_headers)    \n",
    "    for page in tqdm(range(start_page, end_page + 1)):\n",
    "        url = f'https://poradna.finance.cz/?paginator-page={page}&paginator-itemsPerPage=20&do=paginator-paginate'\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error on page {page}: HTTP Status Code {response.status_code}\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        next_questions_div = soup.find('div', class_='nextQuestions')\n",
    "        questions_titles = next_questions_div.find_all('div', class_='responder')\n",
    "        \n",
    "        for root_item in questions_titles:\n",
    "            QA_link = root_item.find_next_sibling('div', class_='dates').find('a', class_='greenText')['href']\n",
    "            QA_response = requests.get(base_url + QA_link)\n",
    "            if QA_response.status_code != 200:\n",
    "                print(f\"Error on page {page}: HTTP Status Code {QA_response.status_code}\")\n",
    "                continue\n",
    "            QA_soup = BeautifulSoup(QA_response.text, 'html.parser')\n",
    "            QA_div = QA_soup.find('div', class_='consulting')\n",
    "            question_title = QA_div.find('h1').text.strip().replace('\\xa0', ' ')\n",
    "            section_span = QA_div.find('span')\n",
    "            section = section_span.find('a').text.strip().replace('\\xa0', ' ')\n",
    "            theme_elements = section_span.find_all('a')\n",
    "            theme = theme_elements[1].text.strip().replace('\\xa0', ' ') if len(theme_elements) > 1 else \"\"\n",
    "            \n",
    "            question_div = QA_soup.find('div', class_='question articleText')\n",
    "            question_date_span = question_div.find('span', class_='date')\n",
    "            question_date = question_date_span.text.strip().replace('\\xa0', ' ')\n",
    "            question_text = question_div.find('p').text.strip().replace('\\xa0', ' ').replace('\\n', ' ')\n",
    "            \n",
    "            answer_div = QA_soup.find('div', class_='answer articleText')\n",
    "            answer_header = answer_div.find('h4')\n",
    "            answer_date_span = answer_header.find('span', class_='responder')\n",
    "            responder_text = answer_date_span.text.strip()\n",
    "            responder_name = responder_text.split(':')[1].split(',')[0].strip().replace('\\xa0', ' ')\n",
    "            answer_date_text = answer_date_span.text.strip()\n",
    "            answer_date = answer_date_text.split(',')[-1].strip().replace('\\xa0', ' ')\n",
    "            answer_text = answer_div.find('p').text.strip().replace('\\xa0', ' ').replace('\\n', ' ')\n",
    "            \n",
    "            writer.writerow([section, theme, question_date, question_title, question_text, QA_link,responder_name, answer_date, answer_text])  \n",
    "        time.sleep(random.uniform(0.5, 1.5))"
   ],
   "id": "70cb52581071e6a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1837 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e9683493fc34ecf8eadc6680754e72d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on page 1443: HTTP Status Code 404\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T09:52:50.436706Z",
     "start_time": "2024-04-04T09:52:50.434617Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "58291abf50651cf5",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
